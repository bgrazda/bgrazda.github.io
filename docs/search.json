[
  {
    "objectID": "delete-later/practice.html",
    "href": "delete-later/practice.html",
    "title": "Here is my level one header",
    "section": "",
    "text": "Here is my level one header\nHere is my first paragraph\nHere is my second paragraph, where you can read more about MEDS.\nThis is very important text!"
  },
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "Blog",
    "section": "",
    "text": "Analyzing Environmental Impacts of the 2017 Thomas Fire\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBreaking the Ice: Arctic Warming\n\n\n\n\n\n\nBrooke Grazda\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGreen Hydrogen Potential in the United States\n\n\n\n\n\n\nBrooke Grazda\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Brooke Grazda",
    "section": "",
    "text": "Hi! My name is Brooke Grazda. I am a San Diego native currently based in Santa Barbara, California. I am an environmental data scientist focused on using programming tools to help inform the efficacy of environmental policy. Keep reading for a little bit more about me!\nI am currently a Masters of Environmental Data Science student at the Bren School of Environmental Science & Management. Previously, I graduated from UC Santa Barbara (UCSB) with a Bachelor of Arts in Environmental Studies and a minor in Applied Psychology. While taking interdisciplinary coursework in food, water, and energy, I struggled to narrow down my interests into one singular niche. Rather, I understood that I was passionate about applying technical skills to environmental problem solving with natural resources.\nDuring my undergrad, I worked at the Institute for Energy Efficiency (IEE) at UCSB as an administrative and marketing student assistant. At IEE, I became fascinated by all the different applications of STEM research to increase energy efficiency. The importance of science communication in combating the climate crisis stood out to me significantly. I was curious to figure out ways to bridge the gap between technical science jargon and the general public’s understanding of environmental solutions. In many instances, data science seemed to be my answer.\nI studied tropical biology and conservation while abroad in Costa Rica, and conducted an independent research project. My research project collected and analyzed the threatened behaviors of reef fish located in the Cuajiniquil Bay, and was a catalyst for pursuing environmental data science.\nPrior to starting my graduate program, I worked in my hometown as an environmental educator at a local nonprofit, I Love A Clean San Diego. Here, I resonated deeply with their mission of waste reduction, outreach, and conservation.\nI am committed to use data driven solutions to educate the public and effectively manage natural resources. I am currently an Environmental Resources Management Intern at the County of Santa Barbara Public Works Department in the Resource Recovery & Waste Management Division. My long term goals are to use data science as a tool to inform policy and advocate for environmental justice. Check out some of my projects!"
  },
  {
    "objectID": "posts/green-hydrogen-blog/green_hydro.html",
    "href": "posts/green-hydrogen-blog/green_hydro.html",
    "title": "Green Hydrogen Potential in the United States",
    "section": "",
    "text": "Background: Hydrogen fuel cells powered by solar and wind power is a huge possibility in the transition for a low carbon society. The Biden administration invested $7 billion in seven green hydrogen hub projects as a pathway for decarbonization and potentially reaching net zero emissions by 2050.\nElectrolysis is the process in which electricity is used to split water molecules into hydrogen and oxygen. There are several potential health implications for this energy process, when powered by natural gas. This process by which steam methane reforming is used emits potent greenhouse gases.\nHowever, electrolysis can also be fueled by renewable power. This is known as green hydrogen, and it does not require fossil fuels nor results in greenhouse gas emissions. According to the Natural Resources Defense Council (NRDC), ’while the technology is still in the early stages, falling renewable energy prices, along with the decreasing costs of the electrolyzers themselves and the clean hydrogen tax credit within the Inflation Reduction Act (IRA), could make this type of electrolysis cost competitive with other methods” (NRDC)."
  },
  {
    "objectID": "posts/green-hydrogen-blog/green_hydro.html#load-libraries",
    "href": "posts/green-hydrogen-blog/green_hydro.html#load-libraries",
    "title": "Green Hydrogen Potential in the United States",
    "section": "Load libraries",
    "text": "Load libraries\n\nlibrary(tidyverse)\nlibrary(here)\nlibrary(janitor)\nlibrary(modelr)\nlibrary(knitr)\nlibrary(broom)\n\noptions(scipen = 999)"
  },
  {
    "objectID": "posts/green-hydrogen-blog/green_hydro.html#import-data",
    "href": "posts/green-hydrogen-blog/green_hydro.html#import-data",
    "title": "Green Hydrogen Potential in the United States",
    "section": "Import data",
    "text": "Import data\n\nsolar_h2_potential &lt;- read_csv(here('posts', 'green-hydrogen-blog', 'data', 'H2_Potential_from_Solar', 'H2_Potential_from_Solar.csv'))\nwind_h2_potential &lt;- read_csv(here('posts', 'green-hydrogen-blog','data', 'H2_Potential_from_Wind', 'H2_Potential_from_Wind.csv'))"
  },
  {
    "objectID": "posts/green-hydrogen-blog/green_hydro.html#about-the-data",
    "href": "posts/green-hydrogen-blog/green_hydro.html#about-the-data",
    "title": "Green Hydrogen Potential in the United States",
    "section": "About the Data",
    "text": "About the Data\nThis data was accessed through the National Renewable Energy Laboratory (NREL). NREL provided two datasets of the hydrogen utility potential generated from both wind and solar. The methodoloy outlined in the metadata mentions how the potential is determined for both the wind and solar power sources. The metadata states in explicit detail that the amount of wind and solar power required to produce 1 kg of Hydrogen is 58.8 kWh. Due to these conversion rates being the same, I was able to compare these resources across one joined dataset.\nThe land use assumption for PV cells states that only 10% of a 40 km × 40 km cell’s land area is assumed to be available for photovoltaic development. Within this 10%, only 30% of the area will be covered by solar panels. The electrolysis system requires 58.8 kWh to produce one kg of hydrogen.\nFor onshore and offshore wind, the metadata identifies that the wind power uses the same production rate of 58.8 kWh/kg. For wind power, the study normalized hydrogen potential by county area (sq km) to ensure comparability across counties of different sizes. The normalization process minimizes biases introduced by geographic area, enabling a clearer understanding of regional potential based on renewable resource availability and efficiency. In other words, the potential to produce hydrogen from wind was normalized by county area to minimize differences in values based on the size of areas."
  },
  {
    "objectID": "posts/green-hydrogen-blog/green_hydro.html#data-wrangling",
    "href": "posts/green-hydrogen-blog/green_hydro.html#data-wrangling",
    "title": "Green Hydrogen Potential in the United States",
    "section": "Data Wrangling",
    "text": "Data Wrangling\nIn this code chunk, I began with a full join of both my solar hydrogen potential and wind hydrogen potential dataframes. For consistency, I used clean_names() with the janitor package. Both dataframes are organized by state, area in square km, population in 2010, and hydrogen potential in kg per year. I grouped by state so that I could take the sum of each column that I am interested in, removing all NA values. I made sure to ungroup() to avoid errors later on. I needed to pivot my data frame into utility potential in kg per year and type of renewable resource that produced the hydrogen.\n\nh2_potential &lt;- full_join(solar_h2_potential, wind_h2_potential) |&gt; \n  clean_names() |&gt; \n   group_by(state) |&gt; \n  summarise_at(vars(population_in_2010, area_sq_km, total_utility_pv_hydrogen_potential_kg_yr, total_onshore_offshore_hydrogen_potential_kg_yr), sum, na.rm = TRUE) |&gt; \n  ungroup() |&gt; \n  pivot_longer(cols = c(total_utility_pv_hydrogen_potential_kg_yr, \n             total_onshore_offshore_hydrogen_potential_kg_yr),\n    names_to = c(\"category\", \"type\"),\n    names_sep = \"_\",\n    values_to = \"value\"\n  ) |&gt; \n   mutate(type = case_when(\n    type == \"utility\" ~ \"pv_solar\",\n    type == \"onshore\" ~ \"wind_onshore_offshore\",\n    TRUE ~ type\n  )) |&gt; \n  rename(h2_potential_kg_yr = value) |&gt; \n  select(!category)"
  },
  {
    "objectID": "posts/green-hydrogen-blog/green_hydro.html#hypothesis-testing",
    "href": "posts/green-hydrogen-blog/green_hydro.html#hypothesis-testing",
    "title": "Green Hydrogen Potential in the United States",
    "section": "Hypothesis Testing",
    "text": "Hypothesis Testing\nGreat! Now that my data is formatted correctly, it is time to start my analyses.\nH0: Type of renewable energy source has no effect on hydrogen fuel potential.\nHA: Type of renewable energy source has an effect on hydrogen fuel potential."
  },
  {
    "objectID": "posts/green-hydrogen-blog/green_hydro.html#test-statistic-difference-of-means",
    "href": "posts/green-hydrogen-blog/green_hydro.html#test-statistic-difference-of-means",
    "title": "Green Hydrogen Potential in the United States",
    "section": "Test Statistic: Difference of means",
    "text": "Test Statistic: Difference of means\n\nutility &lt;- h2_potential |&gt; \n  group_by(type) |&gt; \n  summarize(avg_potential = mean(h2_potential_kg_yr))\n\npoint_estimate_h2 &lt;- utility$avg_potential[2] - utility$avg_potential[1]\n\npoint_estimate_h2\n\n[1] -110387667767\n\n\n\nnull_dist &lt;- replicate(1000, {\n  utility &lt;- h2_potential |&gt; \n  mutate(type = sample(type, n())) |&gt; \n  group_by(type) |&gt; \n  summarize(avg_potential = mean(h2_potential_kg_yr))\n\n  point_estimate_h2 &lt;- utility$avg_potential[2] - utility$avg_potential[1]\n\n  point_estimate_h2\n})\n\nggplot(tibble(null_dist), aes(null_dist)) +\n  geom_histogram(bins = 20, color = 'darkgreen',\n                 fill = NA) +\n  geom_vline(xintercept = point_estimate_h2,\n             color = 'red') +\n  theme_minimal() +\n  labs(title = 'Randomization Test')\n\n\n\n\n\n\n\n\nTo quantify the uncertainty, I created a null distribution as seen above, and made sure to calculate the point estimate with our difference in means. The point estimate is clearly located to the far left of the distribution, so we can see how this compares with our P-Value. Below our P-Value is calculated and it is so significant that it comes out to 0."
  },
  {
    "objectID": "posts/green-hydrogen-blog/green_hydro.html#p-value",
    "href": "posts/green-hydrogen-blog/green_hydro.html#p-value",
    "title": "Green Hydrogen Potential in the United States",
    "section": "P-Value",
    "text": "P-Value\n\nsum(abs(null_dist) &gt; abs(point_estimate_h2)) / \n  length(null_dist)\n\n[1] 0"
  },
  {
    "objectID": "posts/green-hydrogen-blog/green_hydro.html#reject-the-h0",
    "href": "posts/green-hydrogen-blog/green_hydro.html#reject-the-h0",
    "title": "Green Hydrogen Potential in the United States",
    "section": "Reject the H0",
    "text": "Reject the H0\nBased on the results of the analysis, I reject the null hypothesis that the type of renewable energy source has no effect on hydrogen fuel potential. This indicates that the type of renewable energy source significantly influences hydrogen fuel potential."
  },
  {
    "objectID": "posts/green-hydrogen-blog/green_hydro.html#linear-regression-model",
    "href": "posts/green-hydrogen-blog/green_hydro.html#linear-regression-model",
    "title": "Green Hydrogen Potential in the United States",
    "section": "Linear Regression Model",
    "text": "Linear Regression Model\nNow, we want to know how the type of source and population influence the hydrogen fuel potential using a linear model. I decided to use a linear regression model because my dependent variable is numerically trying to predict the hydrogen potential based on population and source type. The two predictors are population in 2010 and source type, which are continuous and categorical.\n\n# Fit linear model\nlinear_model &lt;- summary(lm(h2_potential_kg_yr ~ population_in_2010 + type, data = h2_potential))\n\nprint(linear_model)\n\n\nCall:\nlm(formula = h2_potential_kg_yr ~ population_in_2010 + type, \n    data = h2_potential)\n\nResiduals:\n          Min            1Q        Median            3Q           Max \n-120416082360  -34642132855    -320783583   13920485720  616953246477 \n\nCoefficients:\n                               Estimate    Std. Error t value      Pr(&gt;|t|)    \n(Intercept)                 92563489433   14486503152   6.390 0.00000000546 ***\npopulation_in_2010                 3032          1281   2.368        0.0198 *  \ntypewind_onshore_offshore -110387667766   17306566704  -6.378 0.00000000576 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 87390000000 on 99 degrees of freedom\nMultiple R-squared:  0.3186,    Adjusted R-squared:  0.3048 \nF-statistic: 23.14 on 2 and 99 DF,  p-value: 0.000000005666\n\n\nNote the R-squared here is 0.3048. Pretty low, right? Let’s look at the plot and then we can consider an omitted variable bias.\n\n# Examine the p-values for the \"2010 population\" and \"type\" coefficients\nggplot(h2_potential, aes(population_in_2010,\n                         h2_potential_kg_yr,\n                         color = factor(type))) +\n  geom_point() +\n  geom_smooth(se =FALSE, method = 'lm') +\n  theme_light() +\n  labs(title = 'Green Hydrogen Fuel Potential \\nin the United States using 2010 Population',\n       x = 'Population in 2010',\n       y = 'Hydrogen Potential (kg/year)',\n       color = 'Type of Renewable Energy Source')\n\n\n\n\n\n\n\n\nThis plot shows that solar has a higher overall potential for producing hydrogen fuel based on the population in 2010 across the United States. We can see that even in the data itself, solar has greater values overall. This may be because existing PV cells are way more abundant in amount due to their smaller size and lower production/installation costs, despite wind . Whereas both offshore and onshore wind turbines are extremely large, with a single wind turbine blade equaling the length of a single football field. This not only confirms our initial hypothesis testing, but it confirms our initial question. On average, solar power is likely to have a greater overall potential for producing hydrogen as a renewable energy source than wind.\nAn interesting thing I’d like to point out about this plot is our outlier. You may be wondering, what is that in the top right? This value is the hydrogen potential in kg per year for solar pv cells in Texas. The Texas population is 25,145,561 in 2010. The hydrogen potential for solar in Texas is 785,765,312,659 kg per year. Conversely, the onshore or offshore wind source for hydrogen potential in Texas at this same population is 1,320,799,106 kg per year. Clearly there is a significant difference in the potentials for both technologies likely due to the greater cost incentives and size of existing solar pv cells. This makes intuitive sense because of Texas’s notorious solar resource and lots of land. This makes me to think that perhaps area may be a better predictor of this study.\nThis leads me to my next question regarding an added variable:\n\nAs population and area increases, does the potential for hydrogen fuel increase as well?"
  },
  {
    "objectID": "posts/green-hydrogen-blog/green_hydro.html#omitted-variable-bias",
    "href": "posts/green-hydrogen-blog/green_hydro.html#omitted-variable-bias",
    "title": "Green Hydrogen Potential in the United States",
    "section": "Omitted Variable Bias",
    "text": "Omitted Variable Bias\nNow, that we have seen that solar has a greater overall potential for producing hydrogen, I am curious to see if there is an omitted variable bias in the previous model. I want to include the land area per state to see if there is a stronger predictor.\nNote: The raw data is listed in kg per year, indicating that this is the Total Hydrogen Potential as opposed to the Normalized potential, which would be in kg/yr/km2. With this in mind, it is fair to test if there is an omitted variable bias for area by state due to it not being included to calculate the potential itself.\n\nfit2 &lt;- lm(h2_potential_kg_yr ~ population_in_2010 + type + area_sq_km, data = h2_potential)\nsummary(fit2)\n\n\nCall:\nlm(formula = h2_potential_kg_yr ~ population_in_2010 + type + \n    area_sq_km, data = h2_potential)\n\nResiduals:\n          Min            1Q        Median            3Q           Max \n-169600751896  -34475690163    -431361453   17028955137  559694315767 \n\nCoefficients:\n                               Estimate    Std. Error t value       Pr(&gt;|t|)\n(Intercept)                 71275891445   14772147915   4.825 0.000005142621\npopulation_in_2010                 2408          1216   1.980       0.050491\ntypewind_onshore_offshore -110387667766   16277536316  -6.782 0.000000000904\narea_sq_km                       137234         36792   3.730       0.000321\n                             \n(Intercept)               ***\npopulation_in_2010        .  \ntypewind_onshore_offshore ***\narea_sq_km                ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 82200000000 on 98 degrees of freedom\nMultiple R-squared:  0.4033,    Adjusted R-squared:  0.385 \nF-statistic: 22.08 on 3 and 98 DF,  p-value: 0.00000000005266\n\n\nHere we see that the adjusted R-squared is 0.385. While this correlation is still fairly weak,it increased when area was added. This leads me to believe that perhaps there still is an omitted variable bias that is not included in the data. For future studies, it may be valuable to use data on solar radiation or average wind speeds in each of these states and compare separately by type.\n\nh2_potential |&gt; \n  filter(type == 'wind_onshore_offshore') |&gt; \n  ggplot(aes(x = area_sq_km, \n                         y = h2_potential_kg_yr)) +\n  geom_point() +\n  geom_smooth(method = 'lm', se = FALSE) +\n  theme_classic() +\n  labs(x = 'Area (sq km)',\n       y = 'H2 Potential (kg/year)',\n       title = 'Hydrogen Potential for Onshore and Offshore Wind')\n\n\n\n\n\n\n\n\nBecause our previous combined plot was hard to decipher the values for wind hydrogen potential, I wanted to examine the relationship between just wind and area, and just wind and population in 2010. Using geom_smooth() it is clear that the line is less steep and positive. Let’s look at the distribution between population and Hydrogen potential.\n\nh2_potential |&gt; \n  filter(type == 'wind_onshore_offshore') |&gt; \n  ggplot(aes(x = population_in_2010, \n                         y = h2_potential_kg_yr)) +\n  geom_point() +\n  geom_smooth(method = 'lm', se = FALSE) +\n  theme_classic() +\n  labs(x = 'Population in 2010',\n       y = 'H2 Potential (kg/year)',\n       title = 'Hydrogen Potential i for Onshore and Offshore Wind')\n\n\n\n\n\n\n\n\nHere, we can see that the slope is more positive and looks generally more normally distributed. This makes sense because the population sizes are dynamic while state area is a fixed value per state.\n\nh2_potential |&gt; \n  filter(type == 'pv_solar') |&gt; \n  ggplot(aes(x = area_sq_km, \n                         y = h2_potential_kg_yr)) +\n  geom_point() +\n  geom_smooth(method = 'lm', se = FALSE) +\n  theme_classic() +\n  labs(x = 'Area (sq km)',\n       y = 'H2 Potential (kg/year)',\n       title = 'Hydrogen Potential in US States for Solar PV Sources')\n\n\n\n\n\n\n\n\nOut of curiosity, I wanted to see the distribution of area and hydrogen potential. This is very interesting to me because it appears that the residuals are smaller, with the two outliers as exceptions. Similarly, this line has a positive slope."
  },
  {
    "objectID": "posts/green-hydrogen-blog/green_hydro.html#limitations",
    "href": "posts/green-hydrogen-blog/green_hydro.html#limitations",
    "title": "Green Hydrogen Potential in the United States",
    "section": "Limitations",
    "text": "Limitations\nThis study is limited by the scope of the data. Considering that the data includes the population from 2010, the values of hydrogen potential and population have likely had significant changes since then. The sampling strategy of the data utilized satellite imagery and identified land areas that are designated for wind and solar production, and eliminated small clean energy wind and solar plants that would not generate enough energy to power the hydrogen plants. The limitations of this are that there is not a ton of explanation further about the data and it is generally not clear about their inclusions other than these existing technology designations and the conversion rate. With the new presidency, there may be less incentives for green hydrogen and further studies may need to be conducted to predict more updated potential values."
  },
  {
    "objectID": "posts/green-hydrogen-blog/green_hydro.html#references",
    "href": "posts/green-hydrogen-blog/green_hydro.html#references",
    "title": "Green Hydrogen Potential in the United States",
    "section": "References",
    "text": "References\nResource Assessment for Hydrogen Production. M. Melaina, M. Peneve, and D Heimiller. September 2013. NREL TP-5400-55626"
  },
  {
    "objectID": "posts/thomas-fire-exercises.html#exercise-1-visualizing-aqi-during-the-2017-thomas-fire-in-santa-barbara-county",
    "href": "posts/thomas-fire-exercises.html#exercise-1-visualizing-aqi-during-the-2017-thomas-fire-in-santa-barbara-county",
    "title": "Analyzing Environmental Impacts of the 2017 Thomas Fire",
    "section": "Exercise 1: Visualizing AQI during the 2017 Thomas Fire in Santa Barbara County",
    "text": "Exercise 1: Visualizing AQI during the 2017 Thomas Fire in Santa Barbara County\nAbout: The Thomas Fire was the largest wildifre that California had seen at the time. It expanded and scorched 281,893 acres across Santa Barbara and Ventura Counties, and had imemdiate impacts on air quality and existing habitat. This analysis aims to visualize the impacts and spread of the Thomas Fire utilizing the pandas library, practicing with subsetting values related to AQI.\nHighlights: This analysis practices important techniques such as joining dataframes in pandas using the concat() function. This allows us to aggregate two data frames in an outer join to look at data more broadly. Another technique that is useful in this exercise is the to_datetime() method in pandas. After exploring the data, we can see that the date column is a listed as a string object, which is not suitable for plotting temporal data. This method then allows us to use set_index() to the date. The last step before plotting is crucial to our analysis and visualization because we do not want to see every single date object in the dataframe. Instead, we can use the rolling() method to specify that we want to take the rolling average over five days. In order to do this, we must select the column that holds the air quality data, aqi and apply the rolling() method, passing in the argument for 5 days. We can then specify that we want to take the mean().\nDataset descriptions: I am using the Air Quality Index (AQI) from the Environmental Protection Agency to visualize the impact on the AQI of the 2017 Thomas Fire in Santa Barbara County.\nLink to Github repository:\nFirst step is importing our packages and reading in our data. I am downloading the 2017 and 2018 CSVs from the EPA official website using the pandas function read_csv() from our data folder. os allows us to see the file path for reproducibility.\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport os \nimport rioxarray as rioxr\nimport geopandas as gpd\nimport numpy as np\n\nfrom shapely.geometry import box\n\npd.set_option('display.max_columns', None) \n\n\n# Read in data\naqi_17 = pd.read_csv(os.path.join('data','daily_aqi_by_county_2017.csv'))\naqi_18 = pd.read_csv(os.path.join('data','daily_aqi_by_county_2018.csv'))\n\nGreat! Now, it is important that we combine the data using the concat() method. This allows the two dataframes to be concatenated into one dataframe, which we are going to call aqi. The next step includes cleaning the names into snake_case. How we do this is by selecting the columns, appending this with str.lower() to put every. columns into lowercase. The final step of this code is replacing the spaces with underscores, using str.replace(). Once this is done, all of the columns will be in tidy format.\n\n# Join the data frames, default outer join\naqi = pd.concat([aqi_17, aqi_18])\n\n# Simplify column names to snake_case (lowercase, replace spaces with underscore)\naqi.columns = (aqi.columns\n                  .str.lower()\n                  .str.replace(' ','_'))\n\nFor the AQI data, we want to only look at Santa Barbara. In order to do so, we must subset areas within the county_name column where it equals Santa Barbara. We can create a new dataframe with these values.\n\n# Find all the observations with the county of Santa Barbara and put it into new dataframe \naqi_sb = aqi[aqi['county_name'] == 'Santa Barbara']\n# Remove columns indicated by column labels and axis\naqi_sb = aqi_sb.drop(['state_name', 'county_name', 'state_code', 'county_code'], axis = 1)\n\nPerfect, now we know that we are looking only at Santa Barbara County, so we can drop the other location indicators like state_name, county_name, state_code, and county_code. We can simply pass this list of column names into the drop() method. We must specify that axis = 1, explicitly telling the method that the axis is the columns. For reference, axis = 0 refers to rows, and axis = 1 refers to columns in this method.\nWe are one step closer to plotting. What we want to do is look at the air quality index over time from 2017 to 2018, however the issue is that the actual date column contains strings at its current state. Moreover, we must convert it to a datetime object! When working with temporal data, it is super important to put our dates into this format for ease of plotting and data aggregation. We can just specify the column of the dataframe. To take this even further, we can now set the index as the date, which will be ordered from the beginning of the 2017 data to the end of 2018. Pretty cool!\n\n# Convert date column to datetime object\naqi_sb.date = pd.to_datetime(aqi_sb['date'])\n# Set date column to index\naqi_sb = aqi_sb.set_index('date')\n\nNow it gets a little crazy. It is not always the most visually pleasing to look at the daily AQI data, as it may start to look a little chaotic. In order to account for the amount of data we are dealing with, we can take the rolling average over every 5 days within the time period. The idea is we want to see the rolling average overlaid with the daily object.\nThis is when the rolling() function comes in. Specifying the values of the AQI column, we are able to pass 5D as an argument because we converted the date to our index as a datetime object. Now, calculate the mean() of each value in the Series. We can assign this list to a variable, which we can then further initialize a column with this assigned series called five_day_average\n\n# Calculate AQI rolling average over 5 days\nrolling_average = aqi_sb['aqi'].rolling('5D').mean()\n# Modify and create new column five day average that lines up with the rolling average object. Check to verify.\naqi_sb['five_day_average'] = rolling_average\n\nNow that all those steps are out of the way, we can plot!\n\n\n# Declare fig size so that positioning is clear for labels and captions\nplt.figure(figsize = (10,8))\n\n# Plot both the daily and five day averages for AQI\nplt.plot(aqi_sb.index.values, \n        aqi_sb['aqi'],\n       color = 'brown',\n        label = 'Daily Average AQI')        # Label legend\n\nplt.plot(aqi_sb.index.values,\n        aqi_sb['five_day_average'],\n        color = 'darkblue',\n        label = '5 Day Rolling Average')      # Label legend\n\nplt.xticks(rotation = 30)                 # Rotate xticks so that all dates fit on x axis\nplt.xlabel('Date')                      \nplt.ylabel('AQI')\nplt.title('Santa Barbara Air Quality Index from 2017-2018')\n\n# Assign legend location\nplt.legend(loc=\"upper left\")"
  },
  {
    "objectID": "posts/thomas-fire-exercises.html#exercise-2-false-color-imaging-of-thomas-fire",
    "href": "posts/thomas-fire-exercises.html#exercise-2-false-color-imaging-of-thomas-fire",
    "title": "Analyzing Environmental Impacts of the 2017 Thomas Fire",
    "section": "Exercise 2: False Color Imaging of Thomas Fire",
    "text": "Exercise 2: False Color Imaging of Thomas Fire\nPurpose: This notebook aims to use the landsat raster data of Santa Barbara County and create a false color map image with the Thomas Fire boundary.\nHighlights: A highlight of this exercise was using a NetCDF dataset and dropping the band dimension. Another highlight was subsetting the variables to create a true color image and learning to account for sensitive outliers that may not show up due to cloud coverage. Moreover, using the robust=True argument with both the true color image and false color image was a key step for this. Reprojecting the CRS of both the Thomas Fire boundary and the landsat data was crucial to mapping.\nDataset Description: The dataset is from a California fire perimeter database from Data.gov. The landsat data is Landsat Collection 2 Level-2 atmosperically corrected surface reflectance data, collected by the Landsat 8 satellite. This data was pre-processed to remove data outside land and coarsen the spatial resolution.\nIn our data folder, we have our landsat data that we will be using to create a visualization of Santa Barbara County. We also created a shapefile of the Thomas Fire boundary that we will later overlay on top of the landsat image. The goal here is to use the landsat data and create a false color image to highlight changes indicated by the Thomas Fire boundary.\nWe have already imported the necessary packages at the top of this workflow! Let’s start by importing our data for this exercise.\n\n# Filepath using os for reproducibility\nlandsat = rioxr.open_rasterio(os.path.join('data', \n                                             'landsat8-2018-01-26-sb-simplified.nc'))\n\n# Read in thomas boundary shapefile\nthomas_boundary = gpd.read_file(os.path.join('data', \n                                    'thomas_fire.shp'))\n\n\nPreliminary Landsat Data Exploration\nThe landsat data is an xarray.Dataset with 3 dimensions: band, x, and y. The band dimension has a length of 1 while both the x and y dimensions have lengths of 5. The data variables each show a different pieces that make up the false color imagery including, red, green, blue, nir08 (near infrared, and swir (shortwave infrared). The CRS is EPSG 32611.\n\n\nWrangle Band Data\nAs we discovered in our preliminary data exploration, the landsat is not a data frame. This object is a NetCDF created with a Rioxarray. There are elements to this that make the landsat data unfit for visualization, so we must wrangle the data to get it in the proper format for false color imaging.\n\n# Drop the band dimension of the data\nlandsat = landsat.squeeze().drop_vars('band')\n\nsqueeze() is a method that removes all dimensions of the xarray.Dataset with a length of 1. In this case, the band dimension is a length of 1. The drop_vars() method specifies which element we want to remove, and returns the simplified rioxarray. I then called the dataset as a checkpoint to ensure the functions performed how I intended. Now that the band has been dropped, we can now start to visualize by selecting groups of the electromagnetic spectrum that are used in remote sensing.\n\n\nTrue Color Image\n\n# Select red, green, and blue variables into an array and plot\nlandsat[['red', 'green', 'blue']].to_array().plot.imshow()\n\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n\n\n\n\n\n\n\n\n\nWhat we see here is a visualization landsat data that includes outlying values in our spectral bands. The clouds act as a barrier between the land cover and the satellite, so we cannot see the true color image. The RGB values have outliers, making the image to be squished into what looks like a blank outline of the land area. In the second output, using the robust=True argument adjusts the scale to be more sensitive to outliers, giving us the true RGB image.\n\n# Robust parameters resolve the cloud cover issue\nlandsat[['red', 'green', 'blue']].to_array().plot.imshow(robust=True)\n\n\n\n\n\n\n\n\n\n\nFalse Color Image\nNow, what we notice about the true color image is it does not show subtle changes in the land use due to the bands we selected. False color imaging allows us see these differences more clearly. We can select the short infrared, near infrared, and red spectral bands. Can you guess where the fire happened?\n\n# Select short infrared, near infrared, and red bands to plot\n# Make plot sensitive to outliers with robust = True\nlandsat[['swir22', 'nir08', 'red']].to_array().plot.imshow(robust = True)\n\n\n\n\n\n\n\n\n\n\nMap Thomas Fire Perimeter\nNow, we can overlay our Thomas Fire boundary shapefile with our false imaging. For geospatial data, we must make sure the projections align. I have created a conditional check for the coordinate reference systems. If the thomas boundary and landsat coordinate reference systems are not the same, then the code will reproject both to EPSG:4326 for uniformity.\n\n# Check to make sure CRS's match, if not reproject\nif thomas_boundary.crs != landsat.rio.crs:\n    thomas_boundary= thomas_boundary.to_crs(\"EPSG:4326\")\n    landsat = landsat.rio.reproject(\"EPSG:4326\")\n\nPerfect! We are now ready to plot. We can plot our Thomas Fire boundary on the same axes as the false color image. We can adjust the boundary polygon attributes like color and linewidth how it seems fit.\n\n# Set fig and axes\nfig, ax = plt.subplots(figsize = (8,8))\n\n# Select bands and plot false color\nlandsat[['swir22', 'nir08', 'red']].to_array().plot.imshow(robust = True)\n\n# Plot boundary on the same axes, label the legend \nthomas_boundary.boundary.plot(ax = ax,\n                             color = '#E9190F',\n                             label = 'Thomas Fire Boundary',\n                             linewidth = 2)\n# Show legend\nax.legend()\n\n# Label Title and Axes for cleanliness\nax.set_title(\"False Color Map of California Thomas Fire in 2017\", fontsize=14, fontweight='bold')\nax.set_xlabel('X Coordinate of Projection (m)')\nax.set_ylabel('Y Coordinate of Projection (m)')\n\nText(44.847222222222214, 0.5, 'Y Coordinate of Projection (m)')\n\n\n\n\n\n\n\n\n\n\n\nFigure Description\nThe map above shows a false color image of the land burned by the Thomas Fire in California and surrounding areas. The boundary of the 2017 Thomas Fire is indicated by the red outline on the map. The false color image utilizes the short infrared, near infrared, and red bands in the electromagnetic spectrum to visualize changes in vegetation. Furthermore, the false color imaging shows the secondary succession within the Thomas Fire boundaries, that would not be otherwise noticeable with a True Color Imaging technique. Using this false color, the difference between the land outside of the burned areas is starkly indicated by the green color.\nReferences:\nMicrosoft. Landsat C2 L2. Microsoft Planetary Computer. Retrieved November 12, 2024, from https://planetarycomputer.microsoft.com/dataset/landsat-c2-l2\nPublisher CAL FIRE. (2024, May 14). State of California - California Fire Perimeters (all). Catalog. https://catalog.data.gov/dataset/california-fire-perimeters-all-b3436\nUnited States Environmental Protection Agency. (n.d.). Daily Air Quality Index (AQI) . EPA. https://aqs.epa.gov/aqsweb/airdata/download_files.html#AQI"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Nature, community, and collaboration. I possess a growth mindset to develop practical solutions through a unique approach involving outreach, education, data analysis, and science communication. I believe in reaching beyond barriers and connection to foster productive learning environments. My vision is to work with local governments to create equitable solutions necessary to meet goals in natural resource management."
  },
  {
    "objectID": "about.html#what-inspires-me",
    "href": "about.html#what-inspires-me",
    "title": "About",
    "section": "",
    "text": "Nature, community, and collaboration. I possess a growth mindset to develop practical solutions through a unique approach involving outreach, education, data analysis, and science communication. I believe in reaching beyond barriers and connection to foster productive learning environments. My vision is to work with local governments to create equitable solutions necessary to meet goals in natural resource management."
  },
  {
    "objectID": "about.html#what-im-working-on",
    "href": "about.html#what-im-working-on",
    "title": "About",
    "section": "What I’m working on",
    "text": "What I’m working on\nI’m working locally to achieve SB 1383 goals in the unincorporated Santa Barbara County while completing my Master’s degree. In my academics, I am working on geospatial analyses, statistics, and visualization for environmental data.\nPreviously, I was an environmental educator for a nonprofit in San Diego, delivering presentations and assemblies to students of all ages. These presentations covered topics such as watershed pollution prevention, the role of sea otters within kelp forest ecosystems, food waste, and household hazardous waste."
  },
  {
    "objectID": "about.html#what-is-sb-1383",
    "href": "about.html#what-is-sb-1383",
    "title": "About",
    "section": "What is SB 1383?",
    "text": "What is SB 1383?\nCalifornia State Bill 1383 calls for a 75% reduction statewide in disposal of organic waste by 2025. I work for the County of Santa Barbara Public Works Department in the Resource Recovery and Waste Management Division, working to meet these goals. SB 1383 also madates that 20% of disposed edible food be recovered by 2025. This means all hands on deck! Counties across the state are working to divert organic waste through green bins, recycled food programs, educational presentations in schools, and much, much, more.\nBusiness, residents, and waste haulers are all doing our part to divert our organic waste. Find out more here."
  },
  {
    "objectID": "posts/arctic-infographic/Polar.html#when-given-the-opportunity-to-pick-any-data-source-to-visualize-in-an-infographic-format-i-knew-that-i-wanted-to-do-something-cool",
    "href": "posts/arctic-infographic/Polar.html#when-given-the-opportunity-to-pick-any-data-source-to-visualize-in-an-infographic-format-i-knew-that-i-wanted-to-do-something-cool",
    "title": "Breaking the Ice: Visualizing Arctic Sea Melt",
    "section": "When given the opportunity to pick any data source to visualize in an infographic format, I knew that I wanted to do something ‘cool…’",
    "text": "When given the opportunity to pick any data source to visualize in an infographic format, I knew that I wanted to do something ‘cool…’\nPuns aside, I have always been fascinated with Arctic (even before I adopted Isla, my pet Husky). Climate change is the focal point of my academic and professional interests. With that being said, I knew the findings would be alarming when I first began visualizing arctic data. The Arctic is home to so many fragile and critical ecosystems that are essential indicators of global climate health. While it was apparent to me from the getgo, I was still shocked to see such drastic long term changes in sea ice."
  },
  {
    "objectID": "posts/arctic-infographic/Polar.html",
    "href": "posts/arctic-infographic/Polar.html",
    "title": "Breaking the Ice: Arctic Warming",
    "section": "",
    "text": "Arctic sea ice extent from 1800s to present\n\n\nWhen given the opportunity to pick any data source to visualize in an infographic format, I knew that I wanted to do something “cool…”\nPuns aside, I have always been fascinated with Arctic (even before I adopted Isla, my pet Husky). Climate change is the focal point of my academic and professional interests. With that being said, I knew the findings would be alarming when I first began visualizing arctic data. The Arctic is home to so many fragile and critical ecosystems that are essential indicators of global climate health. While it was apparent to me from the getgo, I was still shocked to see such drastic changes over many years. As a former environmental educator, I wanted to aim this infographic to students or anybody that would be interested in exploring this topic.\nI began with the question: how has Arctic sea ice changed in the last 40 years? This is a very broad question, and there are likely innumerable different pathways to which this question could be answered. Using the NOAA database, I came across a zip file that had so much data on sea ice extent, going all the way back to 1980. I also found a supplementary dataset from the NSF Arctic Data Center on arctic permafrost active layer thickness. The goal of my visualization was to not only answer this question, but tell a visually pleasing, data-driven story about climate change. I was only just dipping my toes in this data, and it was just the tip of the iceberg.\n\n\n\n\n\nInfographic: Breaking the Ice - Data Visualizations on sea ice extent and permafrost active layer\n\n\n\n\n\nWhile I am no expert in arctic data, I sure did have fun making these visualizations! My dataset from NOAA came with monthly images of the arctic circle, so I turned it into a gif using the following code and attached it to the top of this blog post.\nIt felt like a no brainer to me to make a heatmap when I saw that my dataset had daily sea ice data. Using the colorblind friendly Viridis color gradients, I wanted my plots to look as icy as possible. I selected the Mako option when plotting all three of my viz. Looking at the heatmap 20 years apart, it was crazy to see how much the index dropped in September, October, and November. These seasonal fluctuations became a theme in my infographic, as I tried to contextualize my other plots with the same idea. Because it wasn’t clear to me what these index values meant, I used the same images in the gif to display alongside the heatmap.\nThe line plot has yearly data showing these month to month fluctuations, in a different format than before. We can easily see that there is an ebb and flow at different parts of the year, but I wanted to know just how much reduction there was at the lowest point. Using the data from 1980, I was able to calculate that there was a 47.8% reduction in sea ice extent. That statistic was too insane to not include as an annotation!\nI was really excited about including some visualization about permafrost, because it is a striking and extremely interesting issue. This is where I found the supplementary dataset from the NSF Arctic Data Center. I learned a lot about different components of permafrost, mostly the uppermost active layer. As stated in the infographic, this is the uppermost part of the permafrost that freezes and thaws throughout the year. I never even considered that there are wildfires occurring in the Arctic, although in retrospect is rather silly. With increased disturbance, the active layer thickens and creates more thaw in permafrost, allowing warmer temperatures to permeate the ground surface. I selected a stacked area chart with flipped axes to make it look iceberg and icicle-like.\nOne of the hardest decisions to make in this exercise was choosing a typeface. There are so many options, but the AppleGothic typeface was the first one to speak to me. I began using this directly in my plots, but when I began combining all of my plots in Affinity, I needed a bolder face that could highlight certain captions. That is when I found the Apple LiGothic typeface to bolden important components of my infographic. For my themes, I removed all backgrounds so that when I transferred it to Affinity, I could have a uniform slate to work with. This included the type face, mako color scheme, and spacing throughout all plots.\nWhen putting it all on the infographic, it made sense for me to combine the two plots that showed the NOAA sea ice extent data in a stacked format. This gave me limited space on the left to tell the story of permafrost and how it relates to sea ice extent reduction. The underpinning factor in all of my plots is climate change, so I framed each textbox to highlight different aspects of this. My main takeaway is in slightly larger at the bottom, with the entire graphic intentionally meant to be read top to bottom, left to right. This primary message was that this arctic sea ice dilemma is startling because of its furthering impacts on climate change. The organic carbon stored in arctic permafrost releases into the atmosphere once it is exposed through the active layer thickening. The reduction in sea ice is meant to provide context for how fast and when this sea ice changes the most.\nI wanted to frame my question as broadly as possible, and while this data is quite quantitative, the ecosystems and communities that live in these environments are experiencing and seeing these changes firsthand. It is one thing to look at a dataset and visualize it, but to experience such loss is a tragedy that is often overlooked and understudied. I hope that these visualizations are able to provide the viewer with context and inspire an initiative to further the conversation about how climate change is impacting communities in different ways around the globe.\nPlease see below to explore my full code!\n\n\nDisplay Code\nlibrary(tidyverse)\nlibrary(here)\nlibrary(tmap)\nlibrary(sf)\nlibrary(ggExtra)\nlibrary(patchwork)\n\n# Load data\nice_area &lt;- read_csv(here('sea_ice_data', 'sibt_areas_v2.csv'))\nice_extent &lt;-read_csv(here('sea_ice_data', 'sibt_extents_v2.csv')) \n\nice_monthly &lt;- readxl::read_excel(here(\"sea_ice_data\", \"Sea_Ice_Index_Monthly_Data_by_Year_G02135_v3.0.xlsx\"))\n\nroc_arctic &lt;- readxl::read_excel(here(\"sea_ice_data\", \"Sea_Ice_Index_Rates_of_Change_G02135_v3.0.xlsx\"))\n\nshapefile &lt;- read_sf(here('ARPA_polygon', 'ARPA_polygon.shp'))\n\nlatlong &lt;- readxl::read_xlsx(here('sea_ice_data', 'arctic_regions_latlong.xlsx'))\n\nbiomes &lt;- read_csv(here::here('data', 'FireALTEstimatedPairsBurnedUnburned.csv'))\n\ndaily &lt;- readxl::read_xlsx(here('sea_ice_data', 'Sea_Ice_Index_Daily_Extent_G02135_v3.0.xlsx'))\n\n\n# Background color of infographic\ninfo_bg = '#f4f4f9'\n# Tidy daily ice data\n\ndaily_tidy &lt;- daily |&gt; \n  pivot_longer(cols = c(3:53), names_to = 'year', values_to = 'extent_index') |&gt; \n  #filter(!is.na(...1)) |&gt; \n  rename(month = ...1, \n         day = ...2) |&gt; \n  mutate(\n    month = factor(month, levels = month.name, labels = month.abb),  # Optional: Convert to abbreviated month names\n    day = as.numeric(day)  # Ensure day is numeric\n  ) |&gt; \n  fill(month, .direction = \"down\") |&gt; \n  mutate(year = as.numeric(str_extract(year, \"\\\\d+\"))) |&gt; \n  filter(year %in% c(1997, 2017))\n\n# Make heatmap for daily ice data\nggplot(daily_tidy, aes(x = day, y = month, fill = extent_index)) +\n  geom_tile(color = 'white') +  # Use white borders between tiles\n  scale_fill_viridis_c(option = \"mako\", name = \"Extent Index\", direction = -1) +  # Adjust color scale\n  facet_wrap(~year, ncol = 1) +  # Separate heatmaps for each year\n  theme_minimal() +\n  coord_fixed() +                \n  labs(\n    title = \"Daily Arctic Sea Ice Extent: 20 Years Apart\",\n    x = \"Day of the Month\",\n    y = \"\",\n    caption = \"Autumn months show a sharp decline in sea ice extent.\"\n  ) +\n  scale_y_discrete(limits=rev, expand = expansion(mult = c(0.1, 0.1))) +\n theme_void() +\n  theme(\n    axis.text.x = element_text(hjust = 1),  # Rotate x-axis labels for readability\n    legend.position = \"bottom\",\n    legend.title.position = 'top',\n    legend.title = element_text(hjust = .5),\n    text=element_text(size=15,  family=\"AppleGothic\"),\n    plot.caption = element_text(hjust = .5, \n                                margin = margin(0.75, 0, 0, 0, 'cm')),\n    title = element_text(hjust = .5),\n    legend.key.width = unit(2.5, \"cm\"),                 \n    axis.text.y = element_text(size = 10, lineheight = 2, margin = margin(2, 0, 2 ,0, 'cm')),    \n    plot.title = element_text(hjust = 0.5, size = 15,\n                              margin = margin(0,0, 0.3, 0, 'cm')),\n    plot.background = element_rect(fill = info_bg, color = info_bg),\n    panel.background = element_rect(fill = info_bg, color = info_bg),\n    legend.background = element_rect(fill = info_bg, color = info_bg),\n    plot.margin = unit(c(.5,1.5,0,0.5), \"cm\")\n    )     \nggsave(plot = last_plot(), filename = 'daily_sea_ice.svg', width = 10, height = 6)\n\n# Rename first column as \"YYYYDDD\" and the rest based on the first row\ncolnames(ice_extent) &lt;- ice_extent[1,]\nice_extent &lt;- ice_extent[-1,]  # Remove the row used for column names\n# Remove the first row (which contains \"RegnArea\")\nice_area_cleaned &lt;- ice_area[-1, ]\n\n# Convert to long format\nextent_tidy_locs &lt;- ice_extent %&gt;%\n  pivot_longer(cols = 2:18, names_to = \"Region\", values_to = \"ice_extent\") %&gt;%\n  slice(-(1:18)) |&gt;  # Fine to start with year 1850\n  mutate(#YYYDDD = (as.string(YYYYDDD)),\n         ice_extent = as.numeric(ice_extent)) |&gt; # extent in square kilometers \n  mutate(year = sub(\"^(.{4}).*\", \"\\\\1\", YYYYDDD)) |&gt; \n  select(-YYYYDDD) |&gt; \n  group_by(Region, year) |&gt; \n  summarise(ice_ave = mean(ice_extent)) |&gt; \n  ungroup() |&gt; \n  mutate(region = str_trim(Region)) |&gt; \n  filter(Region != 'Northern_Hemisphere') |&gt; \n  left_join(latlong)\n\n# Tidy data\ntidy_monthly_ice &lt;- ice_monthly |&gt; \n  janitor::clean_names() |&gt; \n  rename(year = x1) |&gt; \n  select(-x14) |&gt; \n  pivot_longer(cols = 2:13,\n               names_to = \"month\", \n               values_to = \"ice_extent\")  |&gt; \n   mutate(month = str_to_title(month)) |&gt;  # Capitalize first letter\n  mutate(month = factor(month, levels = month.name, labels = month.abb)) |&gt; \n  arrange(year) |&gt; \n  mutate(percent_change = (ice_extent - lag(ice_extent)) / lag(ice_extent) * 100)\n\n# Filter the rows for January 1980 and January 2020\nice_extent_1980 &lt;- tidy_monthly_ice %&gt;% filter(year == 1980 & month == 'Sep') %&gt;% pull(ice_extent)\nice_extent_2020 &lt;- tidy_monthly_ice %&gt;% filter(year == 2020 & month == 'Sep') %&gt;% pull(ice_extent)\n\n# Calculate the percent change from January 1980 to January 2020\npercent_change_from_1980_to_2020 &lt;- (ice_extent_2020 - ice_extent_1980) / ice_extent_1980 * 100\n\nprint(paste(\"Percent change from September 1980 to September 2020:\", round(percent_change_from_1980_to_2020, 2), \"%\"))\n\n# Line plot with yearly ice data\nggplot(tidy_monthly_ice, aes(x = month, y = ice_extent, group = year, colour = year)) +\n  geom_line(size = .5) +\n  scale_color_viridis_c(option = 'mako', direction = -1) +\n  scale_y_continuous(limits = c(0, NA), expand = c(0, 0)) +\n  theme_classic() +\n  labs(y = 'Sea Ice Extent Index',\n       title = 'Arctic Sea Ice On The Decline',\n       x = ' ',\n       caption = '',\n       color = 'Year') +\n  theme(legend.position = 'bottom',\n        #legend.box.margin = margin(t = 10, r = 10, b = 10, l = 10),  # Adjusts legend margins\n       # plot.margin = margin(t = 15, r = 15, b = 15, l = 15),\n        legend.key.width = unit(3, \"cm\"),  # Stretches legend keys (adjust as needed)\n    #legend.spacing.x = unit(0.5, \"cm\"),\n    text=element_text(size=15,  family=\"AppleGothic\"),\n    legend.title = element_blank(),\n    plot.title = element_text(hjust = 0.5),\n    plot.caption = element_text(hjust = 0.5),\n    plot.background = element_rect(fill = info_bg, color = info_bg),\n    panel.background = element_rect(fill = info_bg, color = info_bg),\n    legend.background = element_rect(fill = info_bg, color = info_bg)\n    )\n\nggsave(plot = last_plot(), filename = 'sea_ice_decline.png', height = 6, width = 8)\n\n# Permafrost plot\nggplot(biomes) +\n  geom_area(aes(x = year, y = estDepth, fill = distur)) +\n scale_fill_manual(values = c('#0B0405FF', '#3487A6FF')) +\n  theme_classic() +\n#  geom_line(data = tidy_monthly_ice, aes(x = year, y =annual), size=2, color = '#023e8a') +\n  theme_classic() +\n  labs(x = ' ',\n       y = 'Estimated ALT Depth (cm)',\n       title = 'Active Layer Thickness (ALT) Disturbance',\n       caption = 'ALT refers to the thickness of the layer above\\npermafrost that freezes and thaws seasonally. \\nData Source: Arctic Data Center',\n       fill = 'Disturbance') +\n  theme(text=element_text(size=13,  family=\"AppleGothic\"),\n       # legend.title = element_text(size = 12, hjust = .5),                                  #sets legend title size\n              legend.position = c(.25, .25),   #sets legend to the top\n     #  legend.position = 'bottom',\n          legend.text = element_text(size = 10),\n        plot.caption = element_text(hjust = .5),\n        panel.border = element_rect(color = \"black\", fill = NA, size = .5),\n     plot.background = element_rect(fill = info_bg, color = info_bg),\n    panel.background = element_rect(fill = info_bg, color = info_bg),\n    legend.background = element_rect(fill = info_bg, color = info_bg)\n     ) +\n  scale_x_continuous(expand = c(0,0), breaks = scales::pretty_breaks(), position = 'top') +  # Move x-axis to the top\n  scale_y_reverse(expand = c(0,0))\n\n# To make the gif!\nlibrary(gifski)\npng_files &lt;- list.files(\"sea_ice_data/images\", pattern = \".*png$\", full.names = TRUE)\ngifski(png_files, gif_file = \"animation.gif\", width = 800, height = 600, delay = 1)"
  },
  {
    "objectID": "posts/arctic-infographic/Polar.html#ice-ice-baby",
    "href": "posts/arctic-infographic/Polar.html#ice-ice-baby",
    "title": "Breaking the Ice: Arctic Warming",
    "section": "",
    "text": "When given the opportunity to pick any data source to visualize in an infographic format, I knew that I wanted to do something ‘cool…’\nPuns aside, I have always been fascinated with Arctic (even before I adopted Isla, my pet Husky). Climate change is the focal point of my academic and professional interests. With that being said, I knew the findings would be alarming when I first began visualizing arctic data. The Arctic is home to so many fragile and critical ecosystems that are essential indicators of global climate health. While it was apparent to me from the getgo, I was still shocked to see such drastic changes over many years.\nI began with the question: how has Arctic sea ice changed in the last 40 years? This is a very broad question, and there are likely innumerable different pathways to which this question could be answered. Using the NOAA database, I came across a zip file that had so much data on sea ice extent, going all the way back to 1980. I also found a supplementary dataset from the NSF Arctic Data Center on arctic permafrost active layer thickness. The goal of my visualization was to not only answer this question, but tell a visually pleasing, data-driven story about climate change. I was only just dipping my toes in this data, and it was just the tip of the iceberg.\n\n\n\nInfographic: Breaking the Ice - Data Visualizations on sea ice extent and permafrost active layer\n\n\nWhile I am no expert in arctic data, I sure did have fun making these visualizations!"
  },
  {
    "objectID": "posts/arctic-infographic/Polar.html#infographic",
    "href": "posts/arctic-infographic/Polar.html#infographic",
    "title": "Breaking the Ice: Arctic Warming",
    "section": "",
    "text": "Infographic: Breaking the Ice - Data Visualizations on sea ice extent and permafrost active layer"
  },
  {
    "objectID": "posts/arctic-infographic/Polar.html#the-process",
    "href": "posts/arctic-infographic/Polar.html#the-process",
    "title": "Breaking the Ice: Arctic Warming",
    "section": "",
    "text": "While I am no expert in arctic data, I sure did have fun making these visualizations! My dataset from NOAA came with monthly images of the arctic circle, so I turned it into a gif using the following code and attached it to the top of this blog post.\nIt felt like a no brainer to me to make a heatmap when I saw that my dataset had daily sea ice data. Using the colorblind friendly Viridis color gradients, I wanted my plots to look as icy as possible. I selected the Mako option when plotting all three of my viz. Looking at the heatmap 20 years apart, it was crazy to see how much the index dropped in September, October, and November. These seasonal fluctuations became a theme in my infographic, as I tried to contextualize my other plots with the same idea. Because it wasn’t clear to me what these index values meant, I used the same images in the gif to display alongside the heatmap.\nThe line plot has yearly data showing these month to month fluctuations, in a different format than before. We can easily see that there is an ebb and flow at different parts of the year, but I wanted to know just how much reduction there was at the lowest point. Using the data from 1980, I was able to calculate that there was a 47.8% reduction in sea ice extent. That statistic was too insane to not include as an annotation!\nI was really excited about including some visualization about permafrost, because it is a striking and extremely interesting issue. This is where I found the supplementary dataset from the NSF Arctic Data Center. I learned a lot about different components of permafrost, mostly the uppermost active layer. As stated in the infographic, this is the uppermost part of the permafrost that freezes and thaws throughout the year. I never even considered that there are wildfires occurring in the Arctic, although in retrospect is rather silly. With increased disturbance, the active layer thickens and creates more thaw in permafrost, allowing warmer temperatures to permeate the ground surface. I selected a stacked area chart with flipped axes to make it look iceberg and icicle-like.\nOne of the hardest decisions to make in this exercise was choosing a typeface. There are so many options, but the AppleGothic typeface was the first one to speak to me. I began using this directly in my plots, but when I began combining all of my plots in Affinity, I needed a bolder face that could highlight certain captions. That is when I found the Apple LiGothic typeface to bolden important components of my infographic. For my themes, I removed all backgrounds so that when I transferred it to Affinity, I could have a uniform slate to work with. This included the type face, mako color scheme, and spacing throughout all plots.\nWhen putting it all on the infographic, it made sense for me to combine the two plots that showed the NOAA sea ice extent data in a stacked format. This gave me limited space on the left to tell the story of permafrost and how it relates to sea ice extent reduction. The underpinning factor in all of my plots is climate change, so I framed each textbox to highlight different aspects of this. My main takeaway is in slightly larger at the bottom, with the entire graphic intentionally meant to be read top to bottom, left to right. This primary message was that this arctic sea ice dilemma is startling because of its furthering impacts on climate change. The organic carbon stored in arctic permafrost releases into the atmosphere once it is exposed through the active layer thickening. The reduction in sea ice is meant to provide context for how fast and when this sea ice changes the most.\nI wanted to frame my question as broadly as possible, and while this data is quite quantitative, the ecosystems and communities that live in these environments are experiencing and seeing these changes firsthand. It is one thing to look at a dataset and visualize it, but to experience such loss is a tragedy that is often overlooked and understudied. I hope that these visualizations are able to provide the viewer with context and inspire an initiative to further the conversation about how climate change is impacting communities in different ways around the globe.\nPlease see below to explore my full code!\n\n\nDisplay Code\nlibrary(tidyverse)\nlibrary(here)\nlibrary(tmap)\nlibrary(sf)\nlibrary(ggExtra)\nlibrary(patchwork)\n\n# Load data\nice_area &lt;- read_csv(here('sea_ice_data', 'sibt_areas_v2.csv'))\nice_extent &lt;-read_csv(here('sea_ice_data', 'sibt_extents_v2.csv')) \n\nice_monthly &lt;- readxl::read_excel(here(\"sea_ice_data\", \"Sea_Ice_Index_Monthly_Data_by_Year_G02135_v3.0.xlsx\"))\n\nroc_arctic &lt;- readxl::read_excel(here(\"sea_ice_data\", \"Sea_Ice_Index_Rates_of_Change_G02135_v3.0.xlsx\"))\n\nshapefile &lt;- read_sf(here('ARPA_polygon', 'ARPA_polygon.shp'))\n\nlatlong &lt;- readxl::read_xlsx(here('sea_ice_data', 'arctic_regions_latlong.xlsx'))\n\nbiomes &lt;- read_csv(here::here('data', 'FireALTEstimatedPairsBurnedUnburned.csv'))\n\ndaily &lt;- readxl::read_xlsx(here('sea_ice_data', 'Sea_Ice_Index_Daily_Extent_G02135_v3.0.xlsx'))\n\n\n# Background color of infographic\ninfo_bg = '#f4f4f9'\n# Tidy daily ice data\n\ndaily_tidy &lt;- daily |&gt; \n  pivot_longer(cols = c(3:53), names_to = 'year', values_to = 'extent_index') |&gt; \n  #filter(!is.na(...1)) |&gt; \n  rename(month = ...1, \n         day = ...2) |&gt; \n  mutate(\n    month = factor(month, levels = month.name, labels = month.abb),  # Optional: Convert to abbreviated month names\n    day = as.numeric(day)  # Ensure day is numeric\n  ) |&gt; \n  fill(month, .direction = \"down\") |&gt; \n  mutate(year = as.numeric(str_extract(year, \"\\\\d+\"))) |&gt; \n  filter(year %in% c(1997, 2017))\n\n# Make heatmap for daily ice data\nggplot(daily_tidy, aes(x = day, y = month, fill = extent_index)) +\n  geom_tile(color = 'white') +  # Use white borders between tiles\n  scale_fill_viridis_c(option = \"mako\", name = \"Extent Index\", direction = -1) +  # Adjust color scale\n  facet_wrap(~year, ncol = 1) +  # Separate heatmaps for each year\n  theme_minimal() +\n  coord_fixed() +                \n  labs(\n    title = \"Daily Arctic Sea Ice Extent: 20 Years Apart\",\n    x = \"Day of the Month\",\n    y = \"\",\n    caption = \"Autumn months show a sharp decline in sea ice extent.\"\n  ) +\n  scale_y_discrete(limits=rev, expand = expansion(mult = c(0.1, 0.1))) +\n theme_void() +\n  theme(\n    axis.text.x = element_text(hjust = 1),  # Rotate x-axis labels for readability\n    legend.position = \"bottom\",\n    legend.title.position = 'top',\n    legend.title = element_text(hjust = .5),\n    text=element_text(size=15,  family=\"AppleGothic\"),\n    plot.caption = element_text(hjust = .5, \n                                margin = margin(0.75, 0, 0, 0, 'cm')),\n    title = element_text(hjust = .5),\n    legend.key.width = unit(2.5, \"cm\"),                 \n    axis.text.y = element_text(size = 10, lineheight = 2, margin = margin(2, 0, 2 ,0, 'cm')),    \n    plot.title = element_text(hjust = 0.5, size = 15,\n                              margin = margin(0,0, 0.3, 0, 'cm')),\n    plot.background = element_rect(fill = info_bg, color = info_bg),\n    panel.background = element_rect(fill = info_bg, color = info_bg),\n    legend.background = element_rect(fill = info_bg, color = info_bg),\n    plot.margin = unit(c(.5,1.5,0,0.5), \"cm\")\n    )     \nggsave(plot = last_plot(), filename = 'daily_sea_ice.svg', width = 10, height = 6)\n\n# Rename first column as \"YYYYDDD\" and the rest based on the first row\ncolnames(ice_extent) &lt;- ice_extent[1,]\nice_extent &lt;- ice_extent[-1,]  # Remove the row used for column names\n# Remove the first row (which contains \"RegnArea\")\nice_area_cleaned &lt;- ice_area[-1, ]\n\n# Convert to long format\nextent_tidy_locs &lt;- ice_extent %&gt;%\n  pivot_longer(cols = 2:18, names_to = \"Region\", values_to = \"ice_extent\") %&gt;%\n  slice(-(1:18)) |&gt;  # Fine to start with year 1850\n  mutate(#YYYDDD = (as.string(YYYYDDD)),\n         ice_extent = as.numeric(ice_extent)) |&gt; # extent in square kilometers \n  mutate(year = sub(\"^(.{4}).*\", \"\\\\1\", YYYYDDD)) |&gt; \n  select(-YYYYDDD) |&gt; \n  group_by(Region, year) |&gt; \n  summarise(ice_ave = mean(ice_extent)) |&gt; \n  ungroup() |&gt; \n  mutate(region = str_trim(Region)) |&gt; \n  filter(Region != 'Northern_Hemisphere') |&gt; \n  left_join(latlong)\n\n# Tidy data\ntidy_monthly_ice &lt;- ice_monthly |&gt; \n  janitor::clean_names() |&gt; \n  rename(year = x1) |&gt; \n  select(-x14) |&gt; \n  pivot_longer(cols = 2:13,\n               names_to = \"month\", \n               values_to = \"ice_extent\")  |&gt; \n   mutate(month = str_to_title(month)) |&gt;  # Capitalize first letter\n  mutate(month = factor(month, levels = month.name, labels = month.abb)) |&gt; \n  arrange(year) |&gt; \n  mutate(percent_change = (ice_extent - lag(ice_extent)) / lag(ice_extent) * 100)\n\n# Filter the rows for January 1980 and January 2020\nice_extent_1980 &lt;- tidy_monthly_ice %&gt;% filter(year == 1980 & month == 'Sep') %&gt;% pull(ice_extent)\nice_extent_2020 &lt;- tidy_monthly_ice %&gt;% filter(year == 2020 & month == 'Sep') %&gt;% pull(ice_extent)\n\n# Calculate the percent change from January 1980 to January 2020\npercent_change_from_1980_to_2020 &lt;- (ice_extent_2020 - ice_extent_1980) / ice_extent_1980 * 100\n\nprint(paste(\"Percent change from September 1980 to September 2020:\", round(percent_change_from_1980_to_2020, 2), \"%\"))\n\n# Line plot with yearly ice data\nggplot(tidy_monthly_ice, aes(x = month, y = ice_extent, group = year, colour = year)) +\n  geom_line(size = .5) +\n  scale_color_viridis_c(option = 'mako', direction = -1) +\n  scale_y_continuous(limits = c(0, NA), expand = c(0, 0)) +\n  theme_classic() +\n  labs(y = 'Sea Ice Extent Index',\n       title = 'Arctic Sea Ice On The Decline',\n       x = ' ',\n       caption = '',\n       color = 'Year') +\n  theme(legend.position = 'bottom',\n        #legend.box.margin = margin(t = 10, r = 10, b = 10, l = 10),  # Adjusts legend margins\n       # plot.margin = margin(t = 15, r = 15, b = 15, l = 15),\n        legend.key.width = unit(3, \"cm\"),  # Stretches legend keys (adjust as needed)\n    #legend.spacing.x = unit(0.5, \"cm\"),\n    text=element_text(size=15,  family=\"AppleGothic\"),\n    legend.title = element_blank(),\n    plot.title = element_text(hjust = 0.5),\n    plot.caption = element_text(hjust = 0.5),\n    plot.background = element_rect(fill = info_bg, color = info_bg),\n    panel.background = element_rect(fill = info_bg, color = info_bg),\n    legend.background = element_rect(fill = info_bg, color = info_bg)\n    )\n\nggsave(plot = last_plot(), filename = 'sea_ice_decline.png', height = 6, width = 8)\n\n# Permafrost plot\nggplot(biomes) +\n  geom_area(aes(x = year, y = estDepth, fill = distur)) +\n scale_fill_manual(values = c('#0B0405FF', '#3487A6FF')) +\n  theme_classic() +\n#  geom_line(data = tidy_monthly_ice, aes(x = year, y =annual), size=2, color = '#023e8a') +\n  theme_classic() +\n  labs(x = ' ',\n       y = 'Estimated ALT Depth (cm)',\n       title = 'Active Layer Thickness (ALT) Disturbance',\n       caption = 'ALT refers to the thickness of the layer above\\npermafrost that freezes and thaws seasonally. \\nData Source: Arctic Data Center',\n       fill = 'Disturbance') +\n  theme(text=element_text(size=13,  family=\"AppleGothic\"),\n       # legend.title = element_text(size = 12, hjust = .5),                                  #sets legend title size\n              legend.position = c(.25, .25),   #sets legend to the top\n     #  legend.position = 'bottom',\n          legend.text = element_text(size = 10),\n        plot.caption = element_text(hjust = .5),\n        panel.border = element_rect(color = \"black\", fill = NA, size = .5),\n     plot.background = element_rect(fill = info_bg, color = info_bg),\n    panel.background = element_rect(fill = info_bg, color = info_bg),\n    legend.background = element_rect(fill = info_bg, color = info_bg)\n     ) +\n  scale_x_continuous(expand = c(0,0), breaks = scales::pretty_breaks(), position = 'top') +  # Move x-axis to the top\n  scale_y_reverse(expand = c(0,0))\n\n# To make the gif!\nlibrary(gifski)\npng_files &lt;- list.files(\"sea_ice_data/images\", pattern = \".*png$\", full.names = TRUE)\ngifski(png_files, gif_file = \"animation.gif\", width = 800, height = 600, delay = 1)"
  }
]